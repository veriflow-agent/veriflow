# orchestrator/llm_output_orchestrator.py
"""
Orchestrator - Fixed Version with Semantic Excerpt Extraction

KEY FIXES:
1. Uses Highlighter for semantic excerpt extraction (instead of keyword matching)
2. Removed the broken _find_relevant_excerpts_in_text() method
3. Increased context window for highlighter
"""

from langsmith import traceable
import time
import asyncio
import os

from utils.html_parser import HTMLParser
from utils.file_manager import FileManager
from utils.logger import fact_logger
from utils.langsmith_config import langsmith_config

# Import your existing components
from agents.browserless_scraper import FactCheckScraper
from agents.fact_checker import FactChecker
from agents.fact_extractor import FactAnalyzer
from agents.highlighter import Highlighter
from utils.job_manager import job_manager

class FactCheckOrchestrator:
    """Orchestrator using global source checking approach with semantic excerpt extraction"""

    def __init__(self, config):
        self.config = config
        self.parser = HTMLParser()
        self.analyzer = FactAnalyzer(config) 
        self.scraper = FactCheckScraper(config)
        self.checker = FactChecker(config)
        self.highlighter = Highlighter(config)  # âœ… NEW: Initialize highlighter
        self.file_manager = FileManager()

        fact_logger.log_component_start("FactCheckOrchestrator")

    @traceable(
        name="fact_check_pipeline",
        run_type="chain",
        tags=["orchestrator", "global-checking", "semantic-extraction"]
    )
    async def process(self, html_content: str) -> dict:
        """
        Improved pipeline with semantic excerpt extraction
        """
        session_id = self.file_manager.create_session()
        start_time = time.time()

        fact_logger.logger.info(
            f"ðŸš€ STARTING FACT-CHECK SESSION: {session_id}",
            extra={"session_id": session_id, "input_length": len(html_content)}
        )

        try:
            # Step 1: Parse input
            fact_logger.logger.info("ðŸ“„ Step 1: Parsing HTML input")
            parsed = await self._traced_parse(html_content)

            # Step 2: Extract facts AND get all source URLs
            fact_logger.logger.info("ðŸ” Step 2: Extracting facts")
            facts, all_source_urls = await self.analyzer.analyze(parsed)

            fact_logger.logger.info(
                f"âœ… Extracted {len(facts)} facts from {len(all_source_urls)} total sources"
            )

            # Step 3: Scrape ALL sources once (no duplicates)
            unique_urls = list(set(all_source_urls))
            fact_logger.logger.info(f"ðŸŒ Step 3: Scraping {len(unique_urls)} unique sources")

            all_scraped_content = await self.scraper.scrape_urls_for_facts(unique_urls)
            successful_scrapes = len([v for v in all_scraped_content.values() if v])

            fact_logger.logger.info(
                f"âœ… Successfully scraped {successful_scrapes}/{len(unique_urls)} sources"
            )

            # Step 4: Save session content and upload to R2
            fact_logger.logger.info("ðŸ’¾ Step 4: Saving session content")
            upload_result = self.file_manager.save_session_content(
                session_id, 
                all_scraped_content,
                facts,
                upload_to_r2=True  # âœ… CHANGED: upload_to_drive â†’ upload_to_r2
            )

            # Log upload status (no job_id in this method, so use logger)
            if upload_result and upload_result.get('success'):
                fact_logger.logger.info("â˜ï¸ Report uploaded to R2")
            else:
                error_msg = upload_result.get('error', 'Unknown error') if upload_result else 'Upload returned no result'
                fact_logger.logger.warning(f"âš ï¸ R2 upload failed: {error_msg}")

            # Step 5: Check each fact using SEMANTIC excerpt extraction
            fact_logger.logger.info(
                f"âš–ï¸ Step 5: Checking {len(facts)} facts with semantic excerpt extraction"
            )
            results = []

            for i, fact in enumerate(facts, 1):
                fact_logger.logger.info(f"Processing fact {i}/{len(facts)}: {fact.id}")

                # âœ… NEW: Use highlighter for semantic excerpt extraction
                excerpts = await self._extract_relevant_excerpts_semantic(
                    fact, 
                    all_scraped_content
                )

                # Check accuracy using your existing fact checker
                check_result = await self.checker.check_fact(fact, excerpts)
                results.append(check_result)

                fact_logger.logger.info(
                    f"âœ… Fact {fact.id} checked: score={check_result.match_score:.2f}",
                    extra={
                        "fact_id": fact.id,
                        "score": check_result.match_score,
                        "excerpts_found": sum(len(v) for v in excerpts.values())
                    }
                )

            # Sort facts by score (lowest first)
            results.sort(key=lambda x: x.match_score)

            # Generate summary
            summary = self._generate_summary(results)
            duration = time.time() - start_time

            fact_logger.logger.info(
                f"ðŸŽ‰ SESSION COMPLETE: {session_id}",
                extra={
                    "session_id": session_id,
                    "duration": duration,
                    "total_facts": len(results),
                    "total_sources": len(unique_urls),
                    "avg_score": summary['avg_score']
                }
            )

            return {
                "session_id": session_id,
                "facts": [r.dict() for r in results],
                "summary": summary,
                "duration": duration,
                "total_sources_scraped": len(unique_urls),
                "successful_scrapes": successful_scrapes,
                "methodology": "global_source_checking_semantic",
                "langsmith_url": f"https://smith.langchain.com/projects/p/{langsmith_config.project_name}"
            }

        except Exception as e:
            fact_logger.log_component_error("FactCheckOrchestrator", e, session_id=session_id)
            raise

    def _check_cancellation(self, job_id: str):
        """Check if job has been cancelled and raise exception if so"""
        from utils.job_manager import job_manager
        if job_manager.is_cancelled(job_id):
            raise Exception("Job cancelled by user")

    async def process_with_progress(self, html_content: str, job_id: str) -> dict:
        """Process with real-time progress updates"""
        from utils.job_manager import job_manager

        session_id = self.file_manager.create_session()
        start_time = time.time()

        try:
            # Step 1: Parse
            job_manager.add_progress(job_id, "ðŸ“„ Parsing HTML input...")
            self._check_cancellation(job_id) 
            parsed = await self._traced_parse(html_content)

            # Step 2: Extract facts
            job_manager.add_progress(job_id, "ðŸ” Extracting verifiable facts...")
            self._check_cancellation(job_id) 
            facts, all_source_urls = await self.analyzer.analyze(parsed)
            job_manager.add_progress(
                job_id, 
                f"âœ… Found {len(facts)} facts from {len(all_source_urls)} sources"
            )

            # Step 3: Scrape sources
            unique_urls = list(set(all_source_urls))
            job_manager.add_progress(
                job_id, 
                f"ðŸŒ Scraping {len(unique_urls)} sources..."
            )
            self._check_cancellation(job_id) 

            all_scraped_content = await self.scraper.scrape_urls_for_facts(unique_urls)
            successful_scrapes = len([v for v in all_scraped_content.values() if v])
            job_manager.add_progress(
                job_id,
                f"âœ… Scraped {successful_scrapes}/{len(unique_urls)} sources"
            )

            # Step 4: Verify each fact
            results = []
            for i, fact in enumerate(facts, 1):
                job_manager.add_progress(
                    job_id,
                    f"âš–ï¸ Verifying fact {i}/{len(facts)}: \"{fact.statement[:60]}...\"",
                    {'fact_id': fact.id, 'progress': f"{i}/{len(facts)}"}
                )
                self._check_cancellation(job_id) 

                excerpts = await self._extract_relevant_excerpts_semantic(fact, all_scraped_content)
                check_result = await self.checker.check_fact(fact, excerpts)
                results.append(check_result)

                # Show result
                emoji = "âœ…" if check_result.match_score >= 0.9 else "âš ï¸" if check_result.match_score >= 0.7 else "âŒ"
                job_manager.add_progress(
                    job_id,
                    f"{emoji} {fact.id}: Score {check_result.match_score:.2f}"
                )
                self._check_cancellation(job_id) 

            results.sort(key=lambda x: x.match_score)

            # Save and finish
            job_manager.add_progress(job_id, "ðŸ’¾ Saving results...")
            self._check_cancellation(job_id) 

            # âœ… CHANGED: Capture upload result (this is a second save, consider removing if duplicate)
            upload_result = self.file_manager.save_session_content(
                session_id, 
                all_scraped_content, 
                facts, 
                upload_to_r2=True  # âœ… CHANGED: upload_to_drive â†’ upload_to_r2
            )

            # âœ… ADD THIS: Show upload status to user
            if upload_result and upload_result.get('success'):
                job_manager.add_progress(job_id, "â˜ï¸ Report uploaded to R2")
            else:
                error_msg = upload_result.get('error', 'Unknown error') if upload_result else 'Upload returned no result'
                job_manager.add_progress(job_id, f"âš ï¸ R2 upload failed: {error_msg}")

            summary = self._generate_summary(results)
            duration = time.time() - start_time

            return {
                "session_id": session_id,
                "facts": [r.dict() for r in results],
                "summary": summary,
                "duration": duration,
                "total_sources_scraped": len(unique_urls),
                "successful_scrapes": successful_scrapes,
                "methodology": "global_source_checking_semantic",
                "langsmith_url": f"https://smith.langchain.com/projects/p/{langsmith_config.project_name}",
                # âœ… NEW: Add R2 upload status
                "r2_upload": {
                    "success": upload_result.get('success', False) if upload_result else False,
                    "url": upload_result.get('url') if upload_result else None,
                    "filename": upload_result.get('filename') if upload_result else None,
                    "error": upload_result.get('error') if upload_result else None
                }
            }

        except Exception as e:
        # Handle cancellation specially
            if "cancelled" in str(e).lower():
                job_manager.add_progress(job_id, "ðŸ›‘ Job cancelled successfully")
                job_manager.fail_job(job_id, "Cancelled by user")
            else:
                fact_logger.log_component_error(f"Job {job_id}", e)
                job_manager.fail_job(job_id, str(e))
            raise

    async def _extract_relevant_excerpts_semantic(
        self, 
        fact, 
        scraped_content: dict
    ) -> dict:
        """
        âœ… NEW: Extract relevant excerpts using semantic understanding via Highlighter

        This replaces the old keyword-based extraction with proper semantic matching.

        Args:
            fact: The Fact object to find excerpts for
            scraped_content: Dict of {url: content}

        Returns:
            Dict of {url: [excerpts]} with semantically matched excerpts
        """
        fact_logger.logger.info(
            f"ðŸ”¦ Using semantic extraction for {fact.id}",
            extra={"fact_id": fact.id, "num_sources": len(scraped_content)}
        )

        # Use the highlighter's semantic extraction for each source
        excerpts_by_url = await self.highlighter.highlight(fact, scraped_content)

        # Log extraction results
        total_excerpts = sum(len(excerpts) for excerpts in excerpts_by_url.values())
        fact_logger.logger.info(
            f"âœ‚ï¸ Semantic extraction complete: {total_excerpts} excerpts from {len(excerpts_by_url)} sources",
            extra={
                "fact_id": fact.id,
                "total_excerpts": total_excerpts,
                "sources_with_matches": len(excerpts_by_url)
            }
        )

        return excerpts_by_url

    @traceable(name="parse_html", run_type="tool")
    async def _traced_parse(self, html_content: str) -> dict:
        """Parse HTML with tracing"""
        return self.parser.parse_input(html_content)

    def _generate_summary(self, results: list) -> dict:
        """Generate summary statistics"""
        if not results:
            return {
                "total_facts": 0,
                "accurate": 0,
                "good_match": 0,
                "questionable": 0,
                "avg_score": 0.0
            }

        total = len(results)
        accurate = len([r for r in results if r.match_score >= 0.9])
        good = len([r for r in results if 0.7 <= r.match_score < 0.9])
        questionable = len([r for r in results if r.match_score < 0.7])
        avg_score = sum(r.match_score for r in results) / total

        return {
            "total_facts": total,
            "accurate": accurate,
            "good_match": good,
            "questionable": questionable,
            "avg_score": round(avg_score, 3)
        }